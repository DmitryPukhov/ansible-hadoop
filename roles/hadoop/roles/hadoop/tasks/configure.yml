---
- name: remove 127.0.1.1 from /etc/hosts
  lineinfile:
    dest=/etc/hosts
    state=absent
    regexp="^127\.0\.1\.1\b"


- name: set JAVA_HOME in .bashrc
  lineinfile:
    dest: '{{ hadoop_user_home }}/.bashrc'
    line: 'export JAVA_HOME={{ java_home }}'
    regexp: '^(# *)?export JAVA_HOME='

- name: set HADOOP_HOME in .bashrc
  lineinfile:
    dest: '{{ hadoop_user_home }}/.bashrc'
    line: 'export HADOOP_HOME={{ hadoop_home }}'
    regexp: '^(# *)?export HADOOP_HOME='

- name: set HADOOP_CONF_DIR in .bashrc
  lineinfile:
    dest: '{{ hadoop_user_home }}/.bashrc'
    line: 'export HADOOP_CONF_DIR={{ hadoop_home }}/conf'
    regexp: '^(# *)?export HADOOP_CONF_DIR='


- name: add PATH to HADOOP_HOME/bin in .bashrc
  lineinfile:
    dest: '{{ hadoop_user_home }}/.bashrc'
    line: 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin # HADOOP-BIN-PATH'
    regexp: '# HADOOP-BIN-PATH'

- name: Ensure hadoop/conf directory exists
  file: path={{hadoop_home}}/conf state=directory

- name: chown to hadoop user
  file:
    dest: '{{ hadoop_home }}/conf'
    owner: '{{ hadoop_user }}'
    group: '{{ hadoop_group }}'
    recurse: yes


- name: copy hadoop-env.sh
  template:
    src: hadoop-env.sh
    dest: '{{ hadoop_home }}/conf/hadoop-env.sh'
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'

- name: copy yarn-env.sh
  template:
    src: yarn-env.sh
    dest: '{{ hadoop_home }}/conf/yarn-env.sh'
    owner: '{{hadoop_user}}'
    force: yes
    group: '{{hadoop_group}}'

  #owner: {{ hadoop_user }}
  #group: {{ hadoop_group }}

#- name: copy hadoop/conf
#  copy: src={{hadoop_home}}/etc/hadoop/hadoop-env.sh dest={{hadoop_home}}/conf/hadoop-env.sh
#  owner: {{hadoop_user}}

#- name: set JAVA_HOME in hadoop-env.sh
#  lineinfile:
#    dest: '{{ hadoop_home }}/conf/hadoop-env.sh'
#    line: 'export JAVA_HOME={{ java_home }}'
#    regexp: '^(# *)?export JAVA_HOME='


#- name: copy hadoop-env.sh
#  lineinfile: dest="{{ hadoop_home }}/etc/hadoop/hadoop-env.sh" regexp=JAVA_HOME= line="export JAVA_HOME={{ java_home }}"


- name: copy yarn-site.xml
  template:
    src: yarn-site.xml
    dest: '{{ hadoop_home }}/conf/yarn-site.xml'
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'

- name: copy hdfs-site.xml
  template:
    src: hdfs-site.xml
    dest: '{{ hadoop_home }}/conf/hdfs-site.xml'
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'


- name: copy core-site.xml
  template:
    src: core-site.xml
    dest: '{{ hadoop_home }}/conf/core-site.xml'
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'

- name: copy capacity-scheduler.xml
  template:
    src: capacity-scheduler.xml
    dest: '{{ hadoop_home }}/conf/capacity-scheduler.xml'
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'

- name: copy slaves
  template:
    src: slaves
    dest: "{{ hadoop_home }}/conf/slaves"
    force: yes
    owner: '{{hadoop_user}}'
    group: '{{hadoop_group}}'
    #notify: restart all
#    notify:
 #     - restart hadoop-dfs
#      - restart yarn-resourcemanager
#      - restart hadoop-datanode
#      - restart yarn-nodemanager
#   #- refreshDFSNodes
   #- refreshYarnNodes
   #- restart hadoop-datanode
   #- restart yarn-nodemanager

#- name: Check for previous hdfs data
#  stat: path=/home/{{hadoop_user}}/data/hdfs
#  register: hdfs_path

- name: delete old hdfs data
  file:
    path: '/home/{{ hadoop_user }}/data/hdfs'
    state: absent

- name: format namenode
  command: "{{ hadoop_home }}/bin/hdfs namenode -format -force -nonInteractive creates=/home/{{ hadoop_user }}/data/hdfs/namenode"
  become: yes
  become_user: '{{ hadoop_user }}'
#  notify:
#    - restart dfs namenode
#    - restart dfs datanode
  #creates=/home/{{ hadoop_user }}/data/hdfs/namenode"
  #when: hdfs_path.stat.exists == false
#- command: "{{ hadoop_home }}/bin/hdfs namenode -format creates=/tmp/hadoop-root/dfs/name"
  #when: (hadoop_type_of_node == 'master') or (hadoop_type_of_node == 'namenode')

